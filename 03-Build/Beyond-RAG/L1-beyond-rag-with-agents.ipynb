{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e58f20a8",
   "metadata": {},
   "source": [
    "---\n",
    "jupyter:\n",
    "  jupytext:\n",
    "    text_representation:\n",
    "      extension: .py\n",
    "      format_name: light\n",
    "      format_version: '1.5'\n",
    "      jupytext_version: 1.16.1\n",
    "  kernelspec:\n",
    "    display_name: Python 3 (ipykernel)\n",
    "    language: python\n",
    "    name: python3\n",
    "# ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01be5e7",
   "metadata": {},
   "source": [
    "# Lab: Beyond RAG - Agentic Systems for Complex Tasks\n",
    "\n",
    "**Goal:** In this lab, you will explore the limitations of standard Retrieval-Augmented Generation (RAG) and learn how to solve more complex problems using an **agentic approach** with tools.\n",
    "\n",
    "**Key Concepts:**\n",
    "- **RAG Limitations:** Understanding that RAG is for knowledge retrieval, not multi-step reasoning.\n",
    "- **Multi-hop Questions:** Identifying questions that require finding and connecting information from multiple sources.\n",
    "- **Agents and Tools:** Using LLMs as reasoning engines that can use tools (functions) to gather information and solve problems step-by-step.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Setup\n",
    "\n",
    "We will need several libraries from the LangChain ecosystem to build our RAG and Agent systems. We'll use a simple in-memory vector store for this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a09346",
   "metadata": {
    "title": "# %pip install langchain langchain-openai python-dotenv faiss-cpu"
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from langchain_openai import OpenAI, OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.agents import AgentExecutor, create_react_agent, tool\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "# Make sure your OPENAI_API_KEY is set in your .env file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747809fe",
   "metadata": {},
   "source": [
    "## 2. The Challenge: A Multi-Document Q&A\n",
    "\n",
    "Our data is spread across three different documents: project descriptions, project assignments, and employee contact info. Our goal is to answer a question that requires information from all three.\n",
    "\n",
    "### Load the Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fab438",
   "metadata": {
    "title": "# Load all documents from the data directory"
   },
   "outputs": [],
   "source": [
    "loader_projects = TextLoader(\"./data/project_docs.txt\")\n",
    "documents_projects = loader_projects.load()\n",
    "\n",
    "loader_assignments = TextLoader(\"./data/assignment_docs.txt\")\n",
    "documents_assignments = loader_assignments.load()\n",
    "\n",
    "loader_employees = TextLoader(\"./data/employee_docs.txt\")\n",
    "documents_employees = loader_employees.load()\n",
    "\n",
    "all_documents = documents_projects + documents_assignments + documents_employees\n",
    "\n",
    "# Split the documents into chunks\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(all_documents)\n",
    "\n",
    "print(f\"Loaded and split {len(texts)} document chunks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cfd88a",
   "metadata": {},
   "source": [
    "## 3. Approach 1: Standard RAG\n",
    "\n",
    "Let's build a standard RAG pipeline. We will embed all the document chunks into a vector store and use a retrieval chain to answer questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40d8821",
   "metadata": {
    "title": "# Create embeddings and the vector store"
   },
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = FAISS.from_documents(texts, embeddings)\n",
    "\n",
    "# Create the RAG chain\n",
    "rag_chain = RetrievalQA.from_chain_type(\n",
    "    llm=OpenAI(),\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore.as_retriever()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7885f9",
   "metadata": {},
   "source": [
    "### Test RAG on a Simple Question\n",
    "A simple question that can be answered from a single retrieved document should work perfectly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d007455",
   "metadata": {
    "simple_question": "What is the description of Project Alpha?",
    "title": "#"
   },
   "outputs": [],
   "source": [
    "# print(f\"Asking RAG: {simple_question}\")\n",
    "# response = rag_chain.invoke(simple_question)\n",
    "# print(f\"Response: {response['result']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c9bd8e",
   "metadata": {},
   "source": [
    "### Test RAG on a Multi-Hop Question\n",
    "Now, let's ask the question that requires \"joining\" information across all three documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3afddea",
   "metadata": {
    "complex_question": "What is the email address of the manager for Project Alpha?",
    "title": "#"
   },
   "outputs": [],
   "source": [
    "# print(f\"Asking RAG: {complex_question}\")\n",
    "# response = rag_chain.invoke(complex_question)\n",
    "# print(f\"Response: {response['result']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a4ad5d",
   "metadata": {},
   "source": [
    "### Why did RAG fail?\n",
    "\n",
    "The RAG pipeline likely failed or gave a vague answer. Here's why:\n",
    "1.  The query \"email address of the manager for Project Alpha\" is semantically closest to the `project_docs.txt` chunk about Project Alpha.\n",
    "2.  The retriever finds that chunk, which says nothing about a manager or an email.\n",
    "3.  The LLM is then asked to answer the question based *only* on that retrieved context and cannot find the answer.\n",
    "\n",
    "RAG is a **fixed pipeline**: `retrieve -> generate`. It cannot perform the multi-step reasoning required:\n",
    "- **Step 1:** Find the manager of Project Alpha (Alice Williams).\n",
    "- **Step 2:** Find the email for Alice Williams.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Approach 2: An Agent with Tools\n",
    "\n",
    "An agent is not a fixed pipeline. It's a reasoning loop (`think -> act -> observe -> think ...`). We can give it **tools** to find the information it needs, step-by-step.\n",
    "\n",
    "### Define Our Tools\n",
    "We'll create simple Python functions that act as our \"APIs\" to the data. The `@tool` decorator makes them usable by a LangChain agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965f9122",
   "metadata": {
    "title": "# For simplicity, we'll just search the raw text of our documents."
   },
   "outputs": [],
   "source": [
    "# # In a real system, these tools would query databases or APIs.\n",
    "\n",
    "# @tool\n",
    "# def get_project_details(project_name: str) -> str:\n",
    "#     \"\"\"Returns the description and status of a given project.\"\"\"\n",
    "#     for doc in documents_projects:\n",
    "#         if project_name.lower() in doc.page_content.lower():\n",
    "#             return doc.page_content\n",
    "#     return \"Project not found.\"\n",
    "\n",
    "# @tool\n",
    "# def get_manager_of_project(project_name: str) -> str:\n",
    "#     \"\"\"Returns the name of the manager for a given project.\"\"\"\n",
    "#     for doc in documents_assignments:\n",
    "#         if project_name.lower() in doc.page_content.lower():\n",
    "#             # Simple parsing for this example\n",
    "#             parts = doc.page_content.split(\" is managed by \")\n",
    "#             return parts[1].replace('.', '')\n",
    "#     return \"Manager not found for that project.\"\n",
    "\n",
    "# @tool\n",
    "# def get_contact_info(employee_name: str) -> str:\n",
    "#     \"\"\"Returns the title and email address for a given employee.\"\"\"\n",
    "#     for doc in documents_employees:\n",
    "#         if employee_name.lower() in doc.page_content.lower():\n",
    "#             return doc.page_content\n",
    "#     return \"Employee contact info not found.\"\n",
    "\n",
    "# tools = [get_project_details, get_manager_of_project, get_contact_info]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cea99f",
   "metadata": {},
   "source": [
    "### Build the Agent\n",
    "We'll use a standard ReAct (Reasoning and Acting) agent. We provide it with the tools and a prompt that tells it how to think and use them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b347bb",
   "metadata": {
    "title": "# The prompt template tells the agent how to reason and use the tools."
   },
   "outputs": [],
   "source": [
    "# prompt_template = \"\"\"\n",
    "# Answer the following questions as best you can. You have access to the following tools:\n",
    "\n",
    "# {tools}\n",
    "\n",
    "# Use the following format:\n",
    "\n",
    "# Question: the input question you must answer\n",
    "# Thought: you should always think about what to do\n",
    "# Action: the action to take, should be one of [{tool_names}]\n",
    "# Action Input: the input to the action\n",
    "# Observation: the result of the action\n",
    "# ... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "# Thought: I now know the final answer\n",
    "# Final Answer: the final answer to the original input question\n",
    "\n",
    "# Begin!\n",
    "\n",
    "# Question: {input}\n",
    "# Thought:{agent_scratchpad}\n",
    "# \"\"\"\n",
    "\n",
    "# prompt = PromptTemplate.from_template(prompt_template)\n",
    "\n",
    "# # Create the agent\n",
    "# agent = create_react_agent(OpenAI(temperature=0), tools, prompt)\n",
    "# agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d28fa1",
   "metadata": {},
   "source": [
    "### Run the Agent on the Complex Question\n",
    "Now, let's give the agent the same multi-hop question that RAG failed on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40628a14",
   "metadata": {
    "incorrectly_encoded_metadata": "response = agent_executor.invoke({\"input\": complex_question})",
    "title": "#"
   },
   "outputs": [],
   "source": [
    "# print(\"\\n--- Agent's Final Response ---\")\n",
    "# print(response['output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a168e3d",
   "metadata": {},
   "source": [
    "### Why did the Agent succeed?\n",
    "\n",
    "Look at the `verbose=True` output. You can see the agent's step-by-step reasoning:\n",
    "1.  **Thought:** \"I need to find the manager of Project Alpha.\"\n",
    "2.  **Action:** `get_manager_of_project`, Input: `\"Project Alpha\"`\n",
    "3.  **Observation:** \"Alice Williams\"\n",
    "4.  **Thought:** \"Now I have the manager's name. I need to find the contact info for Alice Williams.\"\n",
    "5.  **Action:** `get_contact_info`, Input: `\"Alice Williams\"`\n",
    "6.  **Observation:** \"Employee: Alice Williams...\"\n",
    "7.  **Thought:** \"I have all the information. The email is a.williams@aims_pharma.com.\"\n",
    "8.  **Final Answer:** \"The email address of the manager for Project Alpha is a.williams@aims_pharma.com.\"\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "This lab demonstrates the critical difference between RAG and Agentic systems:\n",
    "\n",
    "-   **RAG** is a powerful pattern for **knowledge retrieval**. It's best for when the answer to a question is likely contained within a single, retrievable chunk of text.\n",
    "-   **Agents** are a more general and powerful pattern for **problem-solving**. They are best for complex tasks that require planning, multi-step reasoning, and the ability to \"join\" information from different tools or sources.\n",
    "\n",
    "Understanding when to use each pattern is key to building effective and robust AI applications."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "complex_question,simple_question,incorrectly_encoded_metadata,title,-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
