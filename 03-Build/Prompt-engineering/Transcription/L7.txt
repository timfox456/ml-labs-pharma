One of the exciting things about a large language model is you could use it to build a custom chatbot with only a modest amount of effort. ChatGPT, the web interface, is a way for you to have a conversational interface, a conversation via a large language model. But one of the cool things is you can also use a large language model to build your custom chatbot to maybe play the role of an AI customer service agent or an AI order taker for a restaurant. And in this video, you'll learn how to do that by yourself. I'm going to describe the components of the OpenAI chat completions format in more detail and then you're going to build a chatbot yourself. So let's get into it. So first we'll set up the OpenAI Python package as usual. So chat models like ChatGPT are actually trained to take a series of messages as input and return a model generated message as output. And so although the chat format is designed to make multi-turn conversations like this easy, we've kind of seen through the previous videos that it's also just as useful for single-turn tasks without any conversation. And so next we're going to define two helper functions. So this is the one that we've been using throughout all the videos and it's the "get_completion" function. But if you kind of look at it, we give a prompt but then inside the function what we're actually doing is putting this prompt into what looks like some kind of user message. And this is because the ChatGPT model is a chat model which means it's trained to take a series of messages as input and then return a model generated message as output. So the user message is the input and then the assistant message is the output. So in this video we're going to actually use a different helper function and instead of kind of putting a single prompt as input and getting a single completion, we're going to pass in a list of messages and these messages can be kind of from a variety of different roles. So I'll describe those. So here's an example of a list of messages and so the first message is a system message which kind of gives an overall instruction and then after this message we have turns between the user and the assistant and this will kind of continue to go on. And if you've ever used ChatGPT, the web interface, then your messages are the user messages and then ChatGPT's messages are the assistant messages. So the system message helps to kind of set the behavior and persona of the assistant and it acts as kind of a high-level instruction for the conversation. So you can kind of think of it as whispering in the assistant's ear and kind of guiding its responses without the user being aware of the system message. So as the user, if you've ever used ChatGPT, you probably don't know what's in ChatGPT's system message. The benefit of the system message is that it provides you, the developer, with a way to kind of frame the conversation without making the request itself part of the conversation. So you can kind of guide the assistant and kind of whisper in its ear and guide its responses without making the user aware. So now let's try to use these messages in a conversation. So we'll use our new helper function to get the completion from the messages. And we're also using a higher temperature. So the system message says, "You are an assistant that speaks like Shakespeare". So this is us kind of describing to the assistant how it should behave. And then the first user message is, "Tell me a joke". The next is, "Why did the chicken cross the road?" And then the final user message is, "I don't know." So if we run this, the response is "To get to the other side". Let's try again. "To get to the other side, fair sir or madam". It is an old and classic that never fails. So there's our Shakespearean response. And let's actually try one more thing. Because I want to make it even clearer that this is the assistant message. So here, let's just go and print the entire message response. So just to make this even clearer. This response is an assistant message. So the role is assistant and then the content is the message itself. So that's what's happening in this helper function. We're just kind of passing out the content of the message. So now let's do another example. So here our messages are, the system message is "You are a friendly chatbot", and the first user message is, "Hi, my name is Isa". And we want to get the first user message. So let's execute this for the first assistant message. And so the first message is, "Hello Isa! It's nice to meet you. How can I assist you today?" Now let's try another example. So here our messages are system message, "You are a friendly chatbot" and the first user message is, "Yes, can you remind me what is my "name? And let's get the response. And as you can see, the model doesn't actually know my name. So each conversation with a language model is a standalone interaction, which means that you must provide all relevant messages for the model to draw from in the current conversation. If you want the model to draw from, or quote unquote remember earlier parts of a conversation, you must provide the earlier exchanges in the input to the model. And so we'll refer to this as context. So let's try this. So now we've kind of given the context that the model needs, which is my name, in the previous messages, and we'll ask the same question, so we'll ask what my name is. And the model is able to respond because it has all of the context it needs in this kind of list of messages that we input to it. So now you're going to build your own chatbot. This chatbot is going to be called "OrderBot", and we're going to automate the collection of user prompts and assistant responses in order to build this "OrderBot". And it's going to take orders at a pizza restaurant, so first we're going to define this helper function, and what this is doing is it's going to kind of collect our user messages so we can avoid typing them in by hand in the same, in the way that we did above, and this is going to kind of collect prompts from a user interface that we'll build below, and then append it to a list called "context", and then it will call the model with that context every time. And the model response is then also added to the context, so the kind of model message is added to the context, the user message is added to the context, so on, so it just kind of grows longer and longer. This way the model has the information it needs to determine what to do next. And so now we'll set up and run this kind of UI to display the Autobot. And so here's the context, and it contains the system message that contains the menu. And note that every time we call the language model, we're going to use the same context, and the context is building up over time. And then let's execute this. Okay, I'm going to say, "Hi, I would like to order a pizza". And the assistant says, "Great, what pizza would you like to order? We have pepperoni, cheese, and eggplant pizza." Hmm. "How much are they?", Great, okay, we have the prices. I think I'm feeling a medium eggplant pizza. So as you can imagine, we could continue this conversation. And let's kind of look at what we put in the system message. So, "You are Autobot, an automated service to collect orders for a pizza restaurant. You first greet the customer, then collect the order, and then ask if it's a pick-up or delivery. You wait to collect the entire order, then summarize it and check for a final time if the customer wants to add anything else. If it's a delivery, you can ask for an address. Finally, you collect the payment. Make sure to clarify all options, extras and sizes to uniquely identify the item from the menu. You respond in a short, very conversational, friendly style. The menu includes.", and then, here we have the menu. So, let's go back to our conversation, and let's see if the assistant kind of has been following the instructions. Okay, great, the assistant asks if we want any toppings, which we kind of specified in the system message. So, I think we want no extra toppings. Sure thing. "Is there anything else you would like to order?" Let's get some water. Actually,fries. Small or large? This is great because we kind of asked the assistant in the system message to kind of clarify extras and sides. And so, you get the idea, and please feel free to play with this yourself. You can pause the video and just go ahead and run this in your own notebook on the left. And so, now we can ask the model to create a JSON summary that we could send to the order system based on the conversation. So, we're now appending another system message, which is an instruction, and we're saying, "Create a JSON summary of the previous food order. Itemize the price for each item. The fields should be 1) pizza, include side, 2) list of toppings, 3) list of drinks, and 4) list of sides", and finally, the total price. And you could also use a user message here. This does not have to be a system message. So, let's execute this. And notice, in this case, we're using a lower temperature because for these kinds of tasks, we want the output to be fairly predictable. For a conversational agent, you might want to use a higher temperature. However, in this case, I would maybe use a lower temperature as well because for a customer's assistant chatbot, you might want the output to be a bit more predictable as well. And so, here we have the summary of our order. And so, we could submit this to the order system if we wanted to. So there we have it, you've built your very own order chatbot. Feel free to kind of customise it yourself and play around with the system message to change the behaviour of the chatbot and kind of get it to act as different personas with different knowledge.