{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd470898",
   "metadata": {},
   "source": [
    "---\n",
    "jupyter:\n",
    "  jupytext:\n",
    "    text_representation:\n",
    "      extension: .py\n",
    "      format_name: light\n",
    "      format_version: '1.5'\n",
    "      jupytext_version: 1.16.1\n",
    "  kernelspec:\n",
    "    display_name: Python 3 (ipykernel)\n",
    "    language: python\n",
    "    name: python3\n",
    "# ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d9cf0c",
   "metadata": {},
   "source": [
    "# Final Project 1: Clinical Paper Q&A with RAG\n",
    "\n",
    "**Objective:** Your goal is to build a question-answering system using the Retrieval-Augmented Generation (RAG) pattern. This system will help a researcher quickly find and synthesize information from a collection of clinical paper abstracts on diabetes. \n",
    "\n",
    "**Estimated Time:** 1 hour\n",
    "\n",
    "**Core Task:** You will load several document abstracts, embed them into a vector store, and build a LangChain retrieval chain to answer complex questions. \n",
    "\n",
    "---\n",
    "\n",
    "## 1. Setup\n",
    "\n",
    "First, ensure you have a `.env` file in the project root with your `OPENAI_API_KEY`. Then, install the required packages from `requirements.txt`.\n",
    "\n",
    "```bash\n",
    "# In your terminal\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "Now, let's import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56f317f",
   "metadata": {
    "title": "import pandas as pd"
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_openai import OpenAIEmbeddings, OpenAI\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders.csv_loader import CSVLoader\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2a6459",
   "metadata": {},
   "source": [
    "## 2. Load and Prepare the Data\n",
    "\n",
    "The dataset for this project is `data/diabetes_clinical_papers.csv`. It contains the title and abstract for several clinical papers.\n",
    "\n",
    "**Your Task (Starter Code):**\n",
    "- Use the `CSVLoader` from LangChain to load the documents.\n",
    "- You need to specify which column contains the text you want to embed. For our purposes, the `abstract` is the most important source of information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5348dbd5",
   "metadata": {
    "title": "# Starter Code"
   },
   "outputs": [],
   "source": [
    "loader = CSVLoader(\n",
    "    file_path=\"./data/diabetes_clinical_papers.csv\",\n",
    "    source_column=\"title\", # Use the paper title as the source metadata\n",
    "    csv_args={\"delimiter\": \",\"}\n",
    ")\n",
    "\n",
    "# The 'page_content' of each document will be the text from the 'abstract' column.\n",
    "documents = loader.load()\n",
    "\n",
    "\n",
    "print(f\"Loaded {len(documents)} documents.\")\n",
    "print(\"\\n--- Sample Document ---\")\n",
    "print(documents[0].page_content)\n",
    "print(f\"\\nSource: {documents[0].metadata['source']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5ffd8b",
   "metadata": {},
   "source": [
    "## 3. Build the RAG Pipeline\n",
    "\n",
    "Now it's time to build the core of our system. This involves two main steps:\n",
    "1.  **Embedding and Storage:** Convert the documents into numerical vectors (embeddings) and store them in a searchable vector database (we'll use FAISS for its simplicity).\n",
    "2.  **Retrieval and Generation:** Create a chain that can take a user's question, retrieve the most relevant documents from the vector store, and then use an LLM to generate a final answer based on that retrieved context.\n",
    "\n",
    "**Your Task:**\n",
    "- Create an instance of `OpenAIEmbeddings`.\n",
    "- Use `FAISS.from_documents` to create a vector store from your loaded documents and the embeddings model.\n",
    "- Create a `RetrievalQA` chain using the `from_chain_type` method.\n",
    "  - Use an `OpenAI` LLM.\n",
    "  - The chain type should be `\"stuff\"`.\n",
    "  - The retriever will be created from your FAISS vector store (`vectorstore.as_retriever()`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57e644f",
   "metadata": {
    "title": "# --- YOUR CODE HERE ---"
   },
   "outputs": [],
   "source": [
    "\n",
    "# 1. Create the embeddings model\n",
    "embeddings = None # Replace None with the correct code\n",
    "\n",
    "# 2. Create the FAISS vector store\n",
    "vectorstore = None # Replace None with the correct code\n",
    "\n",
    "# 3. Create the RetrievalQA chain\n",
    "rag_chain = None # Replace None with the correct code\n",
    "\n",
    "# --- END YOUR CODE ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467422a6",
   "metadata": {},
   "source": [
    "## 4. Test Your Q&A System\n",
    "\n",
    "If your RAG pipeline is built correctly, you should be able to ask it complex questions that require synthesizing information from one or more abstracts.\n",
    "\n",
    "**Your Task:**\n",
    "- Ask at least two questions to your `rag_chain`.\n",
    "- **Question 1 (Simple):** Ask a question that can be answered from a single abstract (e.g., \"What is the main finding of the EMPA-REG OUTCOME trial?\").\n",
    "- **Question 2 (Complex):** Ask a question that requires information from multiple abstracts (e.g., \"Which drugs have shown benefits for both glycemic control and weight loss?\").\n",
    "- Print the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5016125b",
   "metadata": {
    "title": "# --- YOUR CODE HERE ---"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Question 1\n",
    "simple_question = \"\"\n",
    "# Add your simple question here\n",
    "simple_answer = None # Use your rag_chain to get the answer\n",
    "\n",
    "print(f\"Q: {simple_question}\")\n",
    "print(f\"A: {simple_answer}\\n\")\n",
    "\n",
    "\n",
    "# Question 2\n",
    "complex_question = \"\"\n",
    "# Add your complex question here\n",
    "complex_answer = None # Use your rag_chain to get the answer\n",
    "\n",
    "print(f\"Q: {complex_question}\")\n",
    "print(f\"A: {complex_answer}\\n\")\n",
    "\n",
    "# --- END YOUR CODE ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63052404",
   "metadata": {},
   "source": [
    "## 5. (Optional) Bonus Challenge\n",
    "\n",
    "The default `RetrievalQA` chain is great, but it doesn't cite its sources. Can you modify the chain to return the source documents it used to generate the answer?\n",
    "\n",
    "**Hint:**\n",
    "- Look at the documentation for `RetrievalQA`. Is there a parameter you can set to make it return the source documents?\n",
    "- You might need to change how you call the chain and how you process the output to see the sources."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "title,-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
