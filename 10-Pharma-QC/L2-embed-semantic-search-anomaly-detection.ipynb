{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68eed5ec",
   "metadata": {},
   "source": [
    "# Lab: Embed Lab Reports for Semantic Search and Anomaly Detection\n",
    "\n",
    "**Goal:** In this lab, you will learn how to convert entire documents into numerical representations called \"embeddings.\" You will then use these embeddings to perform two powerful tasks: semantic search and anomaly detection. \n",
    "\n",
    "**Key Concepts:**\n",
    "- **Embeddings:** Understanding that AI models can represent the \"meaning\" of text as a vector of numbers.\n",
    "- **Semantic Search:** Finding documents based on conceptual similarity, not just keyword matching.\n",
    "- **Anomaly Detection:** Identifying unusual or outlier data points in a dataset, which can be a sign of quality issues.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Setup\n",
    "\n",
    "We will need `openai` for creating embeddings, `numpy` for numerical operations, `scikit-learn` for PCA and distance calculations, and `matplotlib` for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced32a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openai numpy scikit-learn matplotlib python-dotenv\n",
    "\n",
    "import os\n",
    "import openai\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bb9929",
   "metadata": {},
   "source": [
    "## 2. Loading and Preparing the Data\n",
    "\n",
    "We have four lab reports. Three are \"normal,\" and one (`lab_report_04.txt`) is an \"anomaly.\" While it technically passes all specifications, its results are consistently at the edge of the acceptable limits. Our goal is to detect this subtle deviation using AI.\n",
    "\n",
    "First, let's load all four reports into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b569b4",
   "metadata": {
    "lines_to_end_of_cell_marker": 0,
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "report_files = [\n",
    "    \"data/lab_report_01.txt\",\n",
    "    \"data/lab_report_02.txt\",\n",
    "    \"data/lab_report_03.txt\",\n",
    "    \"data/lab_report_04.txt\"  # Our anomaly\n",
    "]\n",
    "\n",
    "reports = []\n",
    "for file_path in report_files:\n",
    "    try:\n",
    "        with open(file_path, \"r\") as f:\n",
    "            reports.append({\"filename\": os.path.basename(file_path), \"text\": f.read()})\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Could not find {file_path}\")\n",
    "\n",
    "print(f\"Loaded {len(reports)} reports.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f4e489",
   "metadata": {},
   "source": [
    "## 3. Generating Embeddings\n",
    "\n",
    "An **embedding** is a vector (a list of numbers) that represents the semantic meaning of a piece of text. We will use OpenAI's `text-embedding-3-small` model to generate an embedding for each lab report. Documents with similar meanings will have vectors that are \"closer\" to each other in multi-dimensional space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c6c6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text, model=\"text-embedding-3-small\"):\n",
    "   text = text.replace(\"\\n\", \" \")\n",
    "   return openai.embeddings.create(input = [text], model=model).data[0].embedding\n",
    "\n",
    "# Generate embeddings for each report\n",
    "print(\"Generating embeddings for each lab report...\")\n",
    "for report in reports:\n",
    "    report['embedding'] = get_embedding(report['text'])\n",
    "    print(f\"  - Generated embedding for {report['filename']}\")\n",
    "\n",
    "print(\"Embeddings generated successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d62e764",
   "metadata": {},
   "source": [
    "## 4. Use Case 1: Semantic Search\n",
    "\n",
    "Now that we have embeddings, we can perform semantic search. Instead of matching keywords, we will search by comparing the *meaning*.\n",
    "\n",
    "Let's ask a question, embed it, and then find the lab report whose embedding is most similar to our question's embedding.\n",
    "\n",
    "**Our Question:** \"Which batch had the highest dissolution rate?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d47e44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Embed the search query\n",
    "search_query = \"Which batch had the highest dissolution rate?\"\n",
    "query_embedding = get_embedding(search_query)\n",
    "\n",
    "print(f\"Search Query: '{search_query}'\")\n",
    "\n",
    "# 2. Calculate the similarity between the query and each report\n",
    "# We use cosine similarity, which measures the cosine of the angle between two vectors.\n",
    "# A value of 1 means they are identical, 0 means they are unrelated.\n",
    "similarities = []\n",
    "for report in reports:\n",
    "    similarity = cosine_similarity([query_embedding], [report['embedding']])[0][0]\n",
    "    similarities.append(similarity)\n",
    "    print(f\"  - Similarity with {report['filename']}: {similarity:.4f}\")\n",
    "\n",
    "# 3. Find the most similar report\n",
    "most_similar_index = np.argmax(similarities)\n",
    "most_similar_report = reports[most_similar_index]\n",
    "\n",
    "print(\"\\n--- Search Result ---\")\n",
    "print(f\"The most relevant report is: {most_similar_report['filename']}\")\n",
    "print(\"\\nReport Content:\")\n",
    "print(most_similar_report['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471c555d",
   "metadata": {},
   "source": [
    "### Exercise 1: Your Own Search\n",
    "\n",
    "**Your Task:** Try a different search query. For example:\n",
    "- \"Find the report from analyst Ben Carter.\"\n",
    "- \"Which batch had the most salicylic acid impurity?\"\n",
    "\n",
    "Run the code below with your new query and see if it finds the correct report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c412ccd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "my_search_query = \"Find the report from analyst Ben Carter.\"\n",
    "my_query_embedding = get_embedding(my_search_query)\n",
    "\n",
    "my_similarities = [cosine_similarity([my_query_embedding], [r['embedding']])[0][0] for r in reports]\n",
    "best_match_index = np.argmax(my_similarities)\n",
    "\n",
    "print(f\"My Search Query: '{my_search_query}'\")\n",
    "print(f\"Best match: {reports[best_match_index]['filename']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d9dfd3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Use Case 2: Anomaly Detection\n",
    "\n",
    "Semantic search is powerful, but the real goal is to proactively identify potential quality issues. We can do this by finding reports that are \"semantically different\" from the norm.\n",
    "\n",
    "Our process will be:\n",
    "1.  Define what \"normal\" looks like by averaging the embeddings of our known-good reports.\n",
    "2.  Calculate the distance of each report from this \"normal\" center.\n",
    "3.  The report with the greatest distance is our anomaly.\n",
    "\n",
    "We will use the first three reports as our \"golden batches\" to establish the norm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dab8806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create a \"mean embedding\" from our normal batches\n",
    "normal_embeddings = [reports[i]['embedding'] for i in range(3)] # Reports 0, 1, 2\n",
    "mean_embedding = np.mean(normal_embeddings, axis=0)\n",
    "\n",
    "print(\"Calculated the mean embedding for 'normal' batches.\")\n",
    "\n",
    "# 2. Calculate the cosine distance of each report from the mean\n",
    "# Cosine Distance = 1 - Cosine Similarity\n",
    "distances = []\n",
    "for report in reports:\n",
    "    similarity = cosine_similarity([report['embedding']], [mean_embedding])[0][0]\n",
    "    distance = 1 - similarity\n",
    "    report['distance_from_norm'] = distance\n",
    "    distances.append(distance)\n",
    "    print(f\"  - Distance for {report['filename']}: {distance:.4f}\")\n",
    "\n",
    "# 3. Find the report with the highest distance\n",
    "most_anomalous_index = np.argmax(distances)\n",
    "most_anomalous_report = reports[most_anomalous_index]\n",
    "\n",
    "print(\"\\n--- Anomaly Detection Result ---\")\n",
    "print(f\"The most anomalous report is: {most_anomalous_report['filename']}\")\n",
    "print(f\"This report is the most semantically different from the 'golden batch' profile.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c09a53e",
   "metadata": {},
   "source": [
    "## 6. Visualizing the Anomaly\n",
    "\n",
    "The distances clearly show that `lab_report_04.txt` is an outlier. But to make this even clearer for human analysts, we can visualize the embeddings.\n",
    "\n",
    "The embeddings have over 1500 dimensions, which we can't plot. We'll use a technique called **Principal Component Analysis (PCA)** to reduce the dimensionality down to 2D so we can create a scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43150d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data for PCA\n",
    "all_embeddings = [r['embedding'] for r in reports]\n",
    "filenames = [r['filename'] for r in reports]\n",
    "\n",
    "# Fit PCA and transform the embeddings to 2D\n",
    "pca = PCA(n_components=2)\n",
    "reduced_embeddings = pca.fit_transform(all_embeddings)\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(10, 7))\n",
    "scatter = plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], c='blue', s=100)\n",
    "\n",
    "# Add labels\n",
    "for i, filename in enumerate(filenames):\n",
    "    # Highlight the anomaly\n",
    "    color = 'red' if \"04\" in filename else 'black'\n",
    "    plt.annotate(filename, (reduced_embeddings[i, 0], reduced_embeddings[i, 1]),\n",
    "                 textcoords=\"offset points\", xytext=(0,10), ha='center', color=color)\n",
    "\n",
    "plt.title(\"Visualizing Lab Report Embeddings with PCA\")\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4455ea",
   "metadata": {},
   "source": [
    "### Interpreting the Chart\n",
    "\n",
    "As you can see, the three \"normal\" reports cluster closely together. The anomalous report, `lab_report_04.txt`, is clearly separated from the main cluster. This visual evidence provides a powerful and intuitive way for a QC analyst to spot potential issues, even when all tests are technically \"passing.\"\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "In this lab, you have moved beyond simple text extraction to understanding the *meaning* behind documents. You have learned how to:\n",
    "1.  Generate embeddings to represent documents as numerical vectors.\n",
    "2.  Use these embeddings to find documents related to a natural language query (Semantic Search).\n",
    "3.  Calculate a \"normal\" profile for a set of documents and find outliers (Anomaly Detection).\n",
    "4.  Visualize high-dimensional data to make anomalies easy to spot.\n",
    "\n",
    "These techniques are fundamental for building proactive, intelligent quality control systems in the pharmaceutical industry."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
