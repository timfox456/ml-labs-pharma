{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b186f981",
   "metadata": {},
   "source": [
    "---\n",
    "jupyter:\n",
    "  jupytext:\n",
    "    text_representation:\n",
    "      extension: .py\n",
    "      format_name: light\n",
    "      format_version: '1.5'\n",
    "      jupytext_version: 1.16.1\n",
    "  kernelspec:\n",
    "    display_name: Python 3 (ipykernel)\n",
    "    language: python\n",
    "    name: python3\n",
    "# ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c86b4e",
   "metadata": {},
   "source": [
    "# Lab 2: Long-Context AI with Hugging Face Jamba\n",
    "\n",
    "**Goal:** In this lab, you will use the actual Jamba model from Hugging Face to process a lengthy, domain-specific document. This provides a real-world example of using a powerful, open-source, long-context model. \n",
    "\n",
    "**Key Concepts:**\n",
    "- **Hugging Face Hub:** Loading models and tokenizers directly from the hub.\n",
    "- **Jamba Model:** Using a real hybrid (Transformer + Mamba) model for inference.\n",
    "- **Long-Context Q&A:** Leveraging the model's large context window to answer questions about a full document.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Setup\n",
    "\n",
    "We need the `transformers` library and its dependencies. We'll also need `accelerate` to help with model loading and `bitsandbytes` for quantization to help fit the model into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9b1783",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers torch accelerate bitsandbytes\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Check for GPU availability\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb5b8f4",
   "metadata": {},
   "source": [
    "## 2. Loading the Jamba Model\n",
    "\n",
    "We will load the Jamba model and its tokenizer directly from the Hugging Face Hub. Jamba is a large model, so we'll use 4-bit quantization (`load_in_4bit=True`) to reduce its memory footprint, which is a common practice for running large models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632e1b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model identifier\n",
    "model_id = \"ai21labs/Jamba-v0.1\"\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Load the model\n",
    "# We use 4-bit quantization to make it runnable on consumer hardware.\n",
    "# This will download the model, which may take some time.\n",
    "try:\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        load_in_4bit=True,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "    print(\"Jamba model loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "    print(\"This may be due to memory constraints. If you are on a GPU with limited VRAM, this model may be too large.\")\n",
    "    model = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f432e173",
   "metadata": {},
   "source": [
    "## 3. Loading the Document\n",
    "\n",
    "We will use the same \"Quality by Design (QbD)\" document from the previous lab. This allows us to directly compare the experience of using a placeholder model with the real thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494dc875",
   "metadata": {
    "lines_to_end_of_cell_marker": 0,
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    with open(\"data/quality_by_design_overview.txt\", \"r\") as f:\n",
    "        qbd_document_text = f.read()\n",
    "    print(\"Successfully loaded the 'Quality by Design' document.\")\n",
    "    print(f\"Document length: {len(qbd_document_text.split())} words\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: Make sure 'data/quality_by_design_overview.txt' exists.\")\n",
    "    qbd_document_text = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4de859",
   "metadata": {},
   "source": [
    "## 4. Full-Context Question Answering with Jamba\n",
    "\n",
    "Now, let's use Jamba's long-context capability to answer a question that requires understanding the entire document. We will format the prompt to clearly separate the context (the document) from the question.\n",
    "\n",
    "### Exercise: Ask a Complex Question\n",
    "\n",
    "**Your Task:** Formulate a prompt to ask Jamba a question about the QbD document. The question should require synthesizing information from different parts of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0559fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_jamba(question, context):\n",
    "    \"\"\"\n",
    "    A helper function to format the prompt and get a response from the Jamba model.\n",
    "    \"\"\"\n",
    "    if not model:\n",
    "        return \"Model not loaded. Cannot proceed.\"\n",
    "        \n",
    "    # Format the prompt with the context and the question\n",
    "    prompt = f\"\"\"\n",
    "<document>\n",
    "{context}\n",
    "</document>\n",
    "\n",
    "Based on the document provided above, please answer the following question:\n",
    "{question}\n",
    "\"\"\"\n",
    "    \n",
    "    # Tokenize the input\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Generate the output\n",
    "    # We set a max length to prevent overly long or runaway responses.\n",
    "    output = model.generate(**inputs, max_new_tokens=250)\n",
    "    \n",
    "    # Decode and print the response\n",
    "    response_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "    # The response will include our prompt, so we find and print just the answer part.\n",
    "    answer_start = response_text.find(\"answer the following question:\") + len(\"answer the following question:\")\n",
    "    answer = response_text[answer_start:].strip()\n",
    "    \n",
    "    return answer\n",
    "\n",
    "if qbd_document_text and model:\n",
    "    # YOUR QUESTION HERE\n",
    "    my_question = \"What is the relationship between Critical Quality Attributes (CQAs) and the Design Space?\"\n",
    "    \n",
    "    print(f\"Asking Jamba: {my_question}\\n\")\n",
    "    answer = ask_jamba(my_question, qbd_document_text)\n",
    "    \n",
    "    print(\"--- Jamba's Answer ---\")\n",
    "    print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff4b285",
   "metadata": {},
   "source": [
    "--- \n",
    "## Conclusion\n",
    "\n",
    "In this lab, you used a real, state-of-the-art hybrid model (Jamba) to perform a long-context task. You have learned how to:\n",
    "\n",
    "1.  Load large, powerful models from the Hugging Face Hub.\n",
    "2.  Use techniques like quantization (`load_in_4bit`) to make these models accessible.\n",
    "3.  Apply a long-context model to a real-world, domain-specific document to answer complex questions.\n",
    "\n",
    "This hands-on experience is a direct application of the concepts discussed in the slides and demonstrates the practical power of architectures that go beyond the standard Transformer."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
