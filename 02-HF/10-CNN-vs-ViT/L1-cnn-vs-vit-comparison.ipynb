{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "955d520f",
   "metadata": {},
   "source": [
    "---\n",
    "jupyter:\n",
    "  jupytext:\n",
    "    text_representation:\n",
    "      extension: .py\n",
    "      format_name: light\n",
    "      format_version: '1.5'\n",
    "      jupytext_version: 1.16.1\n",
    "  kernelspec:\n",
    "    display_name: Python 3 (ipykernel)\n",
    "    language: python\n",
    "    name: python3\n",
    "# ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d81bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Lab: CNN vs. Vision Transformer (ViT) for Image Classification\n",
    "#\n",
    "# **Goal:** In this lab, you will build, train, and evaluate two different deep learning architectures—a classic Convolutional Neural Network (CNN) and a modern Vision Transformer (ViT)—on the same image classification task. This will provide a direct comparison of their performance, complexity, and training characteristics.\n",
    "#\n",
    "# **Key Concepts:**\n",
    "# - **CNN Architecture:** Understanding the role of Conv2D and MaxPooling layers.\n",
    "# - **ViT Architecture:** Understanding how Transformers process images as sequences of patches.\n",
    "# - **Comparative Analysis:** Evaluating models based on metrics like accuracy, parameter count, and training time.\n",
    "#\n",
    "# **Prerequisites:**\n",
    "# - You must have completed the `08-Vision` lab and have the image dataset available.\n",
    "#\n",
    "# ---\n",
    "#\n",
    "# ## 1. Setup and Data Preparation\n",
    "#\n",
    "# First, we need to set up our environment and prepare the dataset.\n",
    "#\n",
    "# **ACTION REQUIRED:** This lab requires the same image dataset from the `08-Vision` lab. Please **copy or symlink** the `data` directory from `labs/02-HF/08-Vision/` into this lab's directory (`labs/02-HF/10-CNN-vs-ViT/`).\n",
    "#\n",
    "# ```bash\n",
    "# # From the root of the project, run:\n",
    "cp -r labs/02-HF/08-Vision/data labs/02-HF/10-CNN-vs-ViT/\n",
    "# ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689d66de",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tensorflow transformers datasets scikit-learn Pillow torch accelerate\n",
    "\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# TensorFlow imports for the CNN\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# PyTorch and Hugging Face imports for the ViT\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import ViTImageProcessor, ViTForImageClassification, TrainingArguments, Trainer\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7da14df",
   "metadata": {},
   "source": [
    "### Load the Dataset\n",
    "We will load the dataset once. We'll use the Hugging Face `datasets` library as it's convenient, and then convert it to a TensorFlow `tf.data.Dataset` for the CNN part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e9662d",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Load with Hugging Face datasets\n",
    "    ds = load_dataset(\"imagefolder\", data_dir=\"data\")\n",
    "    ds = ds['train'].train_test_split(test_size=0.2, seed=42)\n",
    "    \n",
    "    labels = ds[\"train\"].features[\"label\"].names\n",
    "    label2id = {label: i for i, label in enumerate(labels)}\n",
    "    id2label = {i: label for i, label in enumerate(labels)}\n",
    "    NUM_LABELS = len(labels)\n",
    "    \n",
    "    print(f\"Dataset loaded successfully. Labels: {labels}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not load the dataset. Please ensure you have copied the images into the 'data' folder. Error: {e}\")\n",
    "    ds = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d2835b",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Model 1: The Convolutional Neural Network (CNN)\n",
    "\n",
    "First, we'll build and train a standard CNN using TensorFlow/Keras.\n",
    "\n",
    "### Prepare Data for TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195eb662",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 224 # Standard size for many vision models\n",
    "BATCH_SIZE = 8 # Small batch size for limited memory\n",
    "\n",
    "def preprocess_cnn(image, label):\n",
    "    \"\"\"Function to preprocess images for the CNN.\"\"\"\n",
    "    image = tf.image.resize(image, [IMG_SIZE, IMG_SIZE])\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    return image, label\n",
    "\n",
    "if ds:\n",
    "    # Convert Hugging Face dataset to TensorFlow dataset\n",
    "    train_ds_cnn = ds['train'].to_tf_dataset(columns='image', label_cols='label', batch_size=BATCH_SIZE, shuffle=True)\n",
    "    test_ds_cnn = ds['test'].to_tf_dataset(columns='image', label_cols='label', batch_size=BATCH_SIZE)\n",
    "\n",
    "    # Apply preprocessing\n",
    "    train_ds_cnn = train_ds_cnn.map(preprocess_cnn, num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)\n",
    "    test_ds_cnn = test_ds_cnn.map(preprocess_cnn, num_parallel_calls=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44cc854",
   "metadata": {},
   "source": [
    "### Build and Train the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c01888b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'train_ds_cnn' in locals():\n",
    "    # Define the CNN model\n",
    "    cnn_model = keras.Sequential([\n",
    "        layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3)),\n",
    "        layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(NUM_LABELS, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    cnn_model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    print(\"--- CNN Model Summary ---\")\n",
    "    cnn_model.summary()\n",
    "\n",
    "    # Train the model\n",
    "    print(\"\\n--- Training CNN ---\")\n",
    "    start_time = time.time()\n",
    "    cnn_history = cnn_model.fit(train_ds_cnn, epochs=5, validation_data=test_ds_cnn)\n",
    "    cnn_training_time = time.time() - start_time\n",
    "    print(f\"CNN training finished in {cnn_training_time:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c96b829",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Model 2: The Vision Transformer (ViT)\n",
    "\n",
    "Now, we'll build and train a ViT using Hugging Face Transformers. This code is adapted from the `08-Vision` lab.\n",
    "\n",
    "### Prepare Data for ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1ba38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ds:\n",
    "    model_checkpoint = \"google/vit-base-patch16-224-in21k\"\n",
    "    image_processor = ViTImageProcessor.from_pretrained(model_checkpoint)\n",
    "\n",
    "    def transform_vit(example_batch):\n",
    "        inputs = image_processor([x for x in example_batch['image']], return_tensors='pt')\n",
    "        inputs['label'] = example_batch['label']\n",
    "        return inputs\n",
    "\n",
    "    prepared_ds_vit = ds.with_transform(transform_vit)\n",
    "\n",
    "    def collate_fn(batch):\n",
    "        return {\n",
    "            'pixel_values': torch.stack([x['pixel_values'] for x in batch]),\n",
    "            'labels': torch.tensor([x['label'] for x in batch])\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4adbf9a6",
   "metadata": {},
   "source": [
    "### Build and Train the ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdc4d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'prepared_ds_vit' in locals():\n",
    "    vit_model = ViTForImageClassification.from_pretrained(\n",
    "        model_checkpoint,\n",
    "        num_labels=NUM_LABELS,\n",
    "        id2label=id2label,\n",
    "        label2id=label2id,\n",
    "        ignore_mismatched_sizes=True\n",
    "    )\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"vit_vs_cnn_model\",\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        num_train_epochs=3,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        remove_unused_columns=False,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=vit_model,\n",
    "        args=training_args,\n",
    "        train_dataset=prepared_ds_vit[\"train\"],\n",
    "        eval_dataset=prepared_ds_vit[\"test\"],\n",
    "        data_collator=collate_fn,\n",
    "        compute_metrics=lambda p: {\"accuracy\": accuracy_score(p.label_ids, np.argmax(p.predictions, axis=1))},\n",
    "        tokenizer=image_processor,\n",
    "    )\n",
    "\n",
    "    print(\"\\n--- Training ViT ---\")\n",
    "    start_time = time.time()\n",
    "    trainer.train()\n",
    "    vit_training_time = time.time() - start_time\n",
    "    print(f\"ViT training finished in {vit_training_time:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e76527f",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Comparison and Analysis\n",
    "\n",
    "Now that both models are trained, let's compare them on key characteristics.\n",
    "\n",
    "### Exercise: Compare the Models\n",
    "\n",
    "**Your Task:** Evaluate both models on their test sets and fill in the comparison table below with the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bd2b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate CNN\n",
    "if 'cnn_model' in locals():\n",
    "    cnn_loss, cnn_accuracy = cnn_model.evaluate(test_ds_cnn)\n",
    "    cnn_params = cnn_model.count_params()\n",
    "else:\n",
    "    cnn_accuracy, cnn_params = 0, 0\n",
    "\n",
    "# Evaluate ViT\n",
    "if 'trainer' in locals():\n",
    "    vit_eval_results = trainer.evaluate()\n",
    "    vit_accuracy = vit_eval_results['eval_accuracy']\n",
    "    vit_params = vit_model.num_parameters()\n",
    "else:\n",
    "    vit_accuracy, vit_params = 0, 0\n",
    "\n",
    "print(\"--- Comparison Results ---\")\n",
    "print(f\"CNN Model:\")\n",
    "print(f\"  - Test Accuracy: {cnn_accuracy:.4f}\")\n",
    "print(f\"  - Training Time: {cnn_training_time:.2f}s\")\n",
    "print(f\"  - Total Parameters: {cnn_params:,}\")\n",
    "\n",
    "print(f\"\\nViT Model:\")\n",
    "print(f\"  - Test Accuracy: {vit_accuracy:.4f}\")\n",
    "print(f\"  - Training Time: {vit_training_time:.2f}s\")\n",
    "print(f\"  - Total Parameters: {vit_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9961ff67",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### Analysis Questions\n",
    "\n",
    "1.  **Accuracy:** Which model performed better on the test set? (Note: With small datasets and few epochs, results can vary. ViTs often need more data than CNNs to shine).\n",
    "2.  **Model Size:** Look at the \"Total Parameters.\" Which model is larger? What does this imply about its capacity and potential for overfitting?\n",
    "3.  **Training Time:** Which model trained faster? Why might this be the case? (Consider the complexity of the operations in each architecture).\n",
    "\n",
    "### Architectural Differences\n",
    "\n",
    "- **CNNs** use an **inductive bias** for images. They are hard-wired to look for local patterns (edges, textures) and build up a hierarchical understanding. This makes them very data-efficient.\n",
    "- **ViTs** treat an image as a generic sequence of patches. They have less built-in bias and learn the relationships between patches from scratch. This makes them incredibly powerful but often requires more data to learn effectively.\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "In this lab, you directly compared two powerful but fundamentally different vision architectures. You saw that there is no single \"best\" model; the choice depends on the task, the amount of data available, and the computational resources.\n",
    "\n",
    "- For smaller, specific datasets, a well-tuned **CNN** can be highly effective and efficient.\n",
    "- For larger, more diverse datasets, a pre-trained **ViT** can often achieve state-of-the-art performance due to its ability to learn global relationships between different parts of an image."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
