{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "682a5da3",
   "metadata": {},
   "source": [
    "# Pipeline Explained\n",
    "\n",
    "In this lab we will go behind the `pipeline` function and see how it works.\n",
    "\n",
    "#### Lab Goals:\n",
    "\n",
    "* Go deeper into the Pipeline.\n",
    "* Investigate what is going on behind the scenes.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187124a6",
   "metadata": {},
   "source": [
    "## Step 1: Repeat sentiment analysis\n",
    "\n",
    "This is the same high-level abstraction from the previous lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69cc6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers torch\n",
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "classifier(\n",
    "    [\n",
    "        \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "        \"I hate this so much!\",\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290ce81b",
   "metadata": {},
   "source": [
    "* You should obtain this output:\n",
    "\n",
    "```text\n",
    "[{'label': 'POSITIVE', 'score': 0.9598047137260437},\n",
    " {'label': 'NEGATIVE', 'score': 0.9994558095932007}]\n",
    "```\n",
    "\n",
    "---\n",
    "## Behind the Scenes: Tokenizer, Model, and Post-processing\n",
    "\n",
    "The `pipeline` function is composed of three main steps:\n",
    "1.  **Tokenizer**: Converts the raw text into a numerical representation (input IDs or tensors).\n",
    "2.  **Model**: The core transformer model processes the numerical inputs and produces raw outputs, called \"logits\".\n",
    "3.  **Post-processing**: Converts the raw logits into a human-readable format, like probabilities and labels.\n",
    "\n",
    "Let's investigate each step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74e2ed0",
   "metadata": {},
   "source": [
    "## Step 2: Investigate the tokenizer\n",
    "\n",
    "The tokenizer takes our raw text and turns it into numbers the model can understand, adding any special tokens the model requires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd1bb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "# Specify the type of tensors we want to get back (PyTorch tensors 'pt')\n",
    "raw_inputs = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"I hate this so much!\",\n",
    "]\n",
    "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7d9c3c",
   "metadata": {},
   "source": [
    "* The output `inputs` is a dictionary containing `input_ids` and `attention_mask`. These are the tensors that will be fed directly to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405acf7d",
   "metadata": {},
   "source": [
    "## Step 3: Investigate the model\n",
    "\n",
    "Now we pass the tokenized inputs to the model.\n",
    "\n",
    "### The Base Model\n",
    "If we use a base model (`AutoModel`), it returns the final hidden states, which are high-dimensional vectors for each input token. This is useful for feature extraction but not for classification directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f02703",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModel.from_pretrained(checkpoint)\n",
    "\n",
    "# The **inputs syntax unpacks the dictionary into keyword arguments,\n",
    "# equivalent to running `model(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'])`\n",
    "outputs = model(**inputs)\n",
    "print(outputs.last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3683a91e",
   "metadata": {},
   "source": [
    "* The output shape `torch.Size([2, 20, 768])` means: (2 sentences, 20 tokens per sentence, 768 hidden dimensions per token).\n",
    "\n",
    "### The \"Head\" Model\n",
    "To get classifications, we need a model with a \"sequence classification head\" on top of the base model. This head is a small neural network that takes the base model's output and converts it into classification scores (logits)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3381de54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# The output logits now have a much smaller dimensionality.\n",
    "print(outputs.logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ee6d92",
   "metadata": {},
   "source": [
    "* The shape `torch.Size([2, 2])` means: (2 sentences, 2 classification scores per sentence)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8156bf16",
   "metadata": {},
   "source": [
    "## Step 4: Post-Processing the Output\n",
    "\n",
    "The model outputs raw scores called **logits**. These are not probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2eb25f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(outputs.logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8076a290",
   "metadata": {},
   "source": [
    "* The output should be similar to:\n",
    "```text\n",
    "tensor([[-1.5607,  1.6123],\n",
    "        [ 4.1692, -3.3464]], grad_fn=<AddmmBackward>)\n",
    "```\n",
    "\n",
    "To convert these logits into probabilities, we need to apply a **SoftMax** function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b382f724",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9f30c7",
   "metadata": {},
   "source": [
    "* Now we have recognizable probability scores:\n",
    "```text\n",
    "tensor([[4.0195e-02, 9.5980e-01],\n",
    "        [9.9946e-01, 5.4418e-04]], grad_fn=<SoftmaxBackward>)\n",
    "```\n",
    "\n",
    "But which score corresponds to which label? We can find this in the model's configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a691ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.id2label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c0970a",
   "metadata": {},
   "source": [
    "* The output `{0: 'NEGATIVE', 1: 'POSITIVE'}` tells us the mapping.\n",
    "\n",
    "* Now we can conclude that the model predicted the following:\n",
    "    * **First sentence:** NEGATIVE: 0.0402, POSITIVE: 0.9598\n",
    "    * **Second sentence:** NEGATIVE: 0.9995, POSITIVE: 0.0005\n",
    "\n",
    "This is exactly what the `pipeline` function does for us automatically!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
