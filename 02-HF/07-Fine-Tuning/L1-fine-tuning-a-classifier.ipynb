{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34be4b65",
   "metadata": {},
   "source": [
    "---\n",
    "jupyter:\n",
    "  jupytext:\n",
    "    text_representation:\n",
    "      extension: .py\n",
    "      format_name: light\n",
    "      format_version: '1.5'\n",
    "      jupytext_version: 1.16.1\n",
    "  kernelspec:\n",
    "    display_name: Python 3 (ipykernel)\n",
    "    language: python\n",
    "    name: python3\n",
    "# ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e932cf79",
   "metadata": {},
   "source": [
    "# Lab: Fine-Tuning a Transformer for Text Classification\n",
    "\n",
    "**Goal:** In this lab, you will learn the most important skill for adapting AI to a specific domain: **fine-tuning**. You will take a general-purpose pre-trained model and fine-tune it on a custom, pharma-related dataset to create a specialized classifier.\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Datasets Library:** The standard way to load and process data in the Hugging Face ecosystem.\n",
    "- **Fine-Tuning:** The process of updating the weights of a pre-trained model on a new, specific task.\n",
    "- **Trainer API:** A high-level API that handles the entire training loop, including optimization, evaluation, and logging.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Setup\n",
    "\n",
    "We need several libraries from the Hugging Face ecosystem, including `datasets` to load our data and `accelerate` to speed up training. We also need `scikit-learn` to compute metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5dfd5a1",
   "metadata": {
    "title": "# %pip install transformers datasets accelerate scikit-learn torch"
   },
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175307ed",
   "metadata": {},
   "source": [
    "## 2. Loading the Custom Dataset\n",
    "\n",
    "The `datasets` library can load data from many sources, including the Hub and local files like CSVs, JSON, etc. We will load our custom `pharma_text_classification.csv` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc31d56",
   "metadata": {
    "title": "# Load the dataset from the local CSV file"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    dataset = load_dataset('csv', data_files='data/pharma_text_classification.csv')\n",
    "    print(\"Dataset loaded successfully:\")\n",
    "    print(dataset)\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: Make sure 'data/pharma_text_classification.csv' exists.\")\n",
    "    dataset = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2cf766",
   "metadata": {},
   "source": [
    "The dataset is currently a single split called `train`. We need to split it into a `train` set and a `test` set to properly evaluate our model's performance on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e146c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset:\n",
    "    # Split the 'train' split into a 80% train and 20% test set\n",
    "    dataset = dataset['train'].train_test_split(test_size=0.2)\n",
    "    print(\"\\nDataset after splitting:\")\n",
    "    print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb11184",
   "metadata": {},
   "source": [
    "## 3. Preprocessing the Data\n",
    "\n",
    "Our model cannot process raw text. We need to **tokenize** it, converting the text into the numerical IDs the model expects. We'll use the tokenizer from the model we plan to fine-tune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce942662",
   "metadata": {
    "title": "# Load the tokenizer for the model we want to fine-tune"
   },
   "outputs": [],
   "source": [
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "# Create a function to tokenize the text\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "if dataset:\n",
    "    # Apply the tokenization to the entire dataset\n",
    "    tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "    print(\"\\nTokenized dataset sample:\")\n",
    "    print(tokenized_datasets[\"train\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e99ad1",
   "metadata": {},
   "source": [
    "## 4. The Fine-Tuning Process\n",
    "\n",
    "Now we are ready to set up and run the fine-tuning job.\n",
    "\n",
    "### Loading the Model\n",
    "We load `AutoModelForSequenceClassification`. It's crucial that we tell it how many labels we have. We also provide mappings from label IDs (0, 1, 2) to human-readable names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88eee6ca",
   "metadata": {
    "lines_to_end_of_cell_marker": 0,
    "lines_to_next_cell": 1,
    "title": "# Define the label mappings"
   },
   "outputs": [],
   "source": [
    "id2label = {0: \"Regulatory\", 1: \"Manufacturing\", 2: \"Clinical\"}\n",
    "label2id = {\"Regulatory\": 0, \"Manufacturing\": 1, \"Clinical\": 2}\n",
    "\n",
    "# Load the model with the correct number of labels and the mappings\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_checkpoint, num_labels=3, id2label=id2label, label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a700f5",
   "metadata": {},
   "source": [
    "### Defining Metrics\n",
    "During training, we want to see more than just the loss; we want to see human-understandable metrics like accuracy. We define a function to compute these metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b0fcb1",
   "metadata": {
    "lines_to_end_of_cell_marker": 0,
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='weighted')\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bec9ef",
   "metadata": {},
   "source": [
    "### Training Arguments\n",
    "The `TrainingArguments` class lets us configure every aspect of the training process, such as the learning rate, number of epochs, and how often to save or evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c604b37a",
   "metadata": {
    "title": "# Define the training arguments"
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"pharma_classifier_model\", # Where to save the model\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\", # Evaluate at the end of each epoch\n",
    "    save_strategy=\"epoch\",       # Save at the end of each epoch\n",
    "    load_best_model_at_end=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbc1720",
   "metadata": {},
   "source": [
    "### The Trainer\n",
    "The `Trainer` object brings everything together: the model, the arguments, the datasets, the tokenizer, and the metrics function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc9cecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'tokenized_datasets' in locals():\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_datasets[\"train\"],\n",
    "        eval_dataset=tokenized_datasets[\"test\"],\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3f2a26",
   "metadata": {},
   "source": [
    "### Let's Train!\n",
    "Now, we just call `trainer.train()`. The Trainer API handles everything else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312f993d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'trainer' in locals():\n",
    "    print(\"Starting fine-tuning...\")\n",
    "    trainer.train()\n",
    "    print(\"Fine-tuning complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e73612",
   "metadata": {},
   "source": [
    "## 5. Using the Fine-Tuned Model\n",
    "\n",
    "The best version of our model has been saved to the `pharma_classifier_model` directory. We can now use it for inference just like any other model from the Hub, for example, by loading it into a `pipeline`.\n",
    "\n",
    "### Exercise: Test Your Model\n",
    "\n",
    "**Your Task:** Load your fine-tuned model into a `text-classification` pipeline and test it on a new sentence. See if it correctly classifies the domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9e0e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# 1. Define a new sentence to classify\n",
    "new_sentence = \"The new drug application was submitted to the regulatory authority.\"\n",
    "\n",
    "# 2. Create a pipeline with your fine-tuned model\n",
    "# The `model` argument should be the path to your saved model.\n",
    "if 'trainer' in locals():\n",
    "    finetuned_classifier = pipeline(\"text-classification\", model=\"pharma_classifier_model/checkpoint-9\") # Adjust checkpoint number if needed\n",
    "\n",
    "    # 3. Get the prediction\n",
    "    prediction = finetuned_classifier(new_sentence)\n",
    "\n",
    "    print(f\"Sentence: '{new_sentence}'\")\n",
    "    print(f\"Predicted Label: {prediction}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf5adef",
   "metadata": {},
   "source": [
    "---\n",
    "## Conclusion\n",
    "\n",
    "Congratulations! You have successfully fine-tuned a general-purpose Transformer model to become a specialist in classifying pharmaceutical texts. You have learned the standard workflow used by practitioners everywhere:\n",
    "\n",
    "1.  Load a custom dataset using `datasets`.\n",
    "2.  Preprocess and tokenize the data.\n",
    "3.  Set up the `Trainer` with a model, data, and training arguments.\n",
    "4.  Run the training loop.\n",
    "5.  Use the final, specialized model for inference.\n",
    "\n",
    "This process is the key to unlocking the power of large language models for your specific domain and use cases."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "title,-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
