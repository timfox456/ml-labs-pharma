{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01a3a88a",
   "metadata": {},
   "source": [
    "---\n",
    "jupyter:\n",
    "  jupytext:\n",
    "    text_representation:\n",
    "      extension: .py\n",
    "      format_name: light\n",
    "      format_version: '1.5'\n",
    "      jupytext_version: 1.16.1\n",
    "  kernelspec:\n",
    "    display_name: Python 3 (ipykernel)\n",
    "    language: python\n",
    "    name: python3\n",
    "# ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a682b6c",
   "metadata": {},
   "source": [
    "# Lab: Fine-Tuning a Vision Transformer (ViT) for Image Classification\n",
    "\n",
    "**Goal:** In this lab, you will apply your fine-tuning skills to a new domain: computer vision. You will fine-tune a Vision Transformer (ViT) to classify images of different pharmaceutical dosage forms (pills, capsules, tablets).\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Vision Transformer (ViT):** Understanding how the Transformer architecture can be applied to images.\n",
    "- **Image Data Loading:** Using the `datasets` library to work with image folder datasets.\n",
    "- **Feature Extraction:** Processing and augmenting images to prepare them for the model.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Setup\n",
    "\n",
    "We need the same core libraries as before, but with the addition of `Pillow` for image processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d499fc48",
   "metadata": {
    "tags": [
     "skip_test"
    ]
   },
   "outputs": [],
   "source": [
    "%pip install transformers datasets accelerate scikit-learn torch Pillow\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import ViTImageProcessor, ViTForImageClassification, TrainingArguments, Trainer\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5fbc14",
   "metadata": {},
   "source": [
    "## 2. Preparing the Dataset\n",
    "\n",
    "**ACTION REQUIRED:** This lab requires an image dataset. Please source your own images and place them in the following directory structure:\n",
    "\n",
    "```\n",
    "labs/02-HF/08-Vision/data/\n",
    "├── pills/\n",
    "│   ├── pill_01.jpg\n",
    "│   └── pill_02.jpg\n",
    "│   ...\n",
    "├── capsules/\n",
    "│   ├── capsule_01.jpg\n",
    "│   └── capsule_02.jpg\n",
    "│   ...\n",
    "└── tablets/\n",
    "    ├── tablet_01.jpg\n",
    "    └── tablet_02.jpg\n",
    "    ...\n",
    "```\n",
    "\n",
    "For a quick test, you can find 5-10 images for each category from a stock photo website.\n",
    "\n",
    "---\n",
    "\n",
    "Once your data is in place, we can load it using the `datasets` library's `ImageFolder` feature, which automatically infers labels from the folder names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4295dcf0",
   "metadata": {
    "tags": [
     "skip_test"
    ]
   },
   "outputs": [],
   "source": [
    "# Load the dataset from the image folders\n",
    "try:\n",
    "    dataset = load_dataset(\"imagefolder\", data_dir=\"data\")\n",
    "    print(\"Dataset loaded successfully:\")\n",
    "    print(dataset)\n",
    "\n",
    "    # Split the 'train' split into a 80% train and 20% test set\n",
    "    dataset = dataset['train'].train_test_split(test_size=0.2)\n",
    "    print(\"\\nDataset after splitting:\")\n",
    "    print(dataset)\n",
    "\n",
    "    # Get the label names\n",
    "    labels = dataset[\"train\"].features[\"label\"].names\n",
    "    label2id = {label: i for i, label in enumerate(labels)}\n",
    "    id2label = {i: label for i, label in enumerate(labels)}\n",
    "    print(f\"\\nLabels: {id2label}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Could not load the dataset. Please ensure your images are in the correct folder structure. Error: {e}\")\n",
    "    dataset = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f76184a",
   "metadata": {},
   "source": [
    "## 3. Preprocessing the Images\n",
    "\n",
    "Just like text, images need to be preprocessed. A `ViTImageProcessor` (also called a Feature Extractor) will:\n",
    "1.  Resize the image to the model's expected size.\n",
    "2.  Normalize the pixel values.\n",
    "3.  Convert the image into a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234506a5",
   "metadata": {
    "tags": [
     "skip_test"
    ]
   },
   "outputs": [],
   "source": [
    "# Load the feature extractor for the model we want to fine-tune\n",
    "model_checkpoint = \"google/vit-base-patch16-224-in21k\"\n",
    "image_processor = ViTImageProcessor.from_pretrained(model_checkpoint)\n",
    "\n",
    "# Create a transformation function\n",
    "def transform(example_batch):\n",
    "    # Take a list of PIL images and turn them to pixel values\n",
    "    inputs = image_processor([x for x in example_batch['image']], return_tensors='pt')\n",
    "    # Don't forget to include the labels!\n",
    "    inputs['label'] = example_batch['label']\n",
    "    return inputs\n",
    "\n",
    "if dataset:\n",
    "    # Apply the transformation\n",
    "    prepared_ds = dataset.with_transform(transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e32d7e",
   "metadata": {},
   "source": [
    "## 4. The Fine-Tuning Process\n",
    "\n",
    "This process is nearly identical to the text fine-tuning lab, demonstrating the power and consistency of the Hugging Face ecosystem.\n",
    "\n",
    "### Loading the Model\n",
    "We load `ViTForImageClassification`, providing it with the number of labels and the label mappings from our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad55add2",
   "metadata": {
    "lines_to_end_of_cell_marker": 0,
    "lines_to_next_cell": 1,
    "tags": [
     "skip_test"
    ]
   },
   "outputs": [],
   "source": [
    "if 'labels' in locals():\n",
    "    model = ViTForImageClassification.from_pretrained(\n",
    "        model_checkpoint,\n",
    "        num_labels=len(labels),\n",
    "        id2label=id2label,\n",
    "        label2id=label2id,\n",
    "        ignore_mismatched_sizes=True # Needed to replace the head of the pre-trained model\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ae8b93",
   "metadata": {},
   "source": [
    "### Training Arguments and Metrics\n",
    "These are the same as before. We define our training configuration and a function to compute accuracy and F1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baaf7182",
   "metadata": {
    "tags": [
     "skip_test"
    ]
   },
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='weighted')\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    return {'accuracy': acc, 'f1': f1, 'precision': precision, 'recall': recall}\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"A custom collator to handle the already-tensorized image data.\"\"\"\n",
    "    return {\n",
    "        'pixel_values': torch.stack([x['pixel_values'] for x in batch]),\n",
    "        'labels': torch.tensor([x['label'] for x in batch])\n",
    "    }\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"pharma_image_classifier_model\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    remove_unused_columns=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc3d951",
   "metadata": {},
   "source": [
    "### The Trainer\n",
    "We bring everything together in the `Trainer` object. Note the use of a custom `collate_fn` because our data is already processed into tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f60eaf8",
   "metadata": {
    "tags": [
     "skip_test"
    ]
   },
   "outputs": [],
   "source": [
    "if 'prepared_ds' in locals():\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=prepared_ds[\"train\"],\n",
    "        eval_dataset=prepared_ds[\"test\"],\n",
    "        tokenizer=image_processor, # For vision models, the processor is passed here\n",
    "        compute_metrics=compute_metrics,\n",
    "        data_collator=collate_fn,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ac2624",
   "metadata": {},
   "source": [
    "### Let's Train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689362fd",
   "metadata": {
    "tags": [
     "skip_test"
    ]
   },
   "outputs": [],
   "source": [
    "if 'trainer' in locals():\n",
    "    print(\"Starting fine-tuning...\")\n",
    "    trainer.train()\n",
    "    print(\"Fine-tuning complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc061713",
   "metadata": {},
   "source": [
    "## 5. Using the Fine-Tuned Model\n",
    "\n",
    "Now you can use your specialized image classifier!\n",
    "\n",
    "### Exercise: Test Your Model\n",
    "\n",
    "**Your Task:** Find one new image of a pill, capsule, or tablet that was **not** in your training data. Use your fine-tuned model to classify it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b06ca1",
   "metadata": {
    "tags": [
     "skip_test"
    ]
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from PIL import Image\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# 1. Provide the path to your new test image\n",
    "#    (You will need to upload this image to your lab environment)\n",
    "# test_image_path = \"path/to/your/new_image.jpg\"\n",
    "\n",
    "# try:\n",
    "#     # 2. Load the image\n",
    "#     image = Image.open(test_image_path)\n",
    "\n",
    "#     # 3. Create a pipeline with your fine-tuned model\n",
    "#     if 'trainer' in locals():\n",
    "#         # Adjust checkpoint number based on your training output\n",
    "#         vision_classifier = pipeline(\"image-classification\", model=\"pharma_image_classifier_model/checkpoint-500\") \n",
    "\n",
    "#         # 4. Get the prediction\n",
    "#         prediction = vision_classifier(image)\n",
    "\n",
    "#         print(f\"Image: '{test_image_path}'\")\n",
    "#         print(f\"Predicted Label: {prediction}\")\n",
    "\n",
    "# except FileNotFoundError:\n",
    "#     print(f\"Please update the 'test_image_path' to a valid image file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d714f769",
   "metadata": {},
   "source": [
    "---\n",
    "## Conclusion\n",
    "\n",
    "You have now successfully fine-tuned a Transformer model for both text and vision! This lab demonstrated that the core workflow is remarkably consistent across different modalities. You learned to:\n",
    "\n",
    "1.  Load a custom image dataset from folders.\n",
    "2.  Use a `ViTImageProcessor` to prepare images for the model.\n",
    "3.  Fine-tune a ViT model using the same `Trainer` API you used for text.\n",
    "4.  Use the final, specialized model for inference on new images.\n",
    "\n",
    "This powerful and flexible workflow allows you to adapt state-of-the-art models to a vast range of tasks in the pharmaceutical domain, from document analysis to visual quality control."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "tags,-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
