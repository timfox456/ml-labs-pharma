{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab: Predicting type of cloth using Convolutional Neural Network (CNN)\n",
    "\n",
    "Fashion MNIST is a drop in replacement for MNIST!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About Fashion mnist\n",
    "\n",
    "Fashion-MNIST is a dataset of Zalando’s article images — consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes.\n",
    "\n",
    "In other words, we have 70,000 images of 28 pixels width and 28 pixels height in greyscale. Each image is showing one of 10 possible clothing types. \n",
    "\n",
    "Here is one:\n",
    "\n",
    "<img src=\"https://elephantscale-public.s3.amazonaws.com/media/machine-learning/fashion-mnist-1.png\">\n",
    "\n",
    "Here are some images from the dataset along with the clothing they are showing:\n",
    "\n",
    "<img src=\"https://elephantscale-public.s3.amazonaws.com/media/machine-learning/fashion-mnist-1.png\">\n",
    "\n",
    "Here are all different types of clothing:\n",
    "\n",
    "\n",
    "\n",
    "| Label | Description |\n",
    "|-------|-------------|\n",
    "| 0     | T-shirt/top |\n",
    "| 1     | Trouser     |\n",
    "| 2     | Pullover    |\n",
    "| 3     | Dress       |\n",
    "| 4     | Coat        |\n",
    "| 5     | Sandal      |\n",
    "| 6     | Shirt       |\n",
    "| 7     | Sneaker     |\n",
    "| 8     | Bag         |\n",
    "| 9     | Ankle boot  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in Google COLAB :  False\n"
     ]
    }
   ],
   "source": [
    "## Determine if we are running on google colab\n",
    "\n",
    "try:\n",
    "    import google.colab\n",
    "    RUNNING_IN_COLAB = True\n",
    "except:\n",
    "    RUNNING_IN_COLAB = False\n",
    "\n",
    "print (\"Running in Google COLAB : \", RUNNING_IN_COLAB)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  # %tensorflow_version only exists in Colab.\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "  pass\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_images shape :  (60000, 28, 28)\n",
      "train_labels shape :  (60000,)\n",
      "test_images shape :  (10000, 28, 28)\n",
      "test_labels shape :  (10000,)\n"
     ]
    }
   ],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "# backup\n",
    "(train_images2, train_labels2), (test_images2, test_labels2) = keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "print(\"train_images shape : \", train_images.shape)\n",
    "print(\"train_labels shape : \", train_labels.shape)\n",
    "print(\"test_images shape : \", test_images.shape)\n",
    "print(\"test_labels shape : \", test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 : Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying train index =  43774\n",
      "train label [43774] = 1 \n",
      "------------ raw data for train_image[43774] -------\n",
      "[[  0   0   0   0   0   0   0   0   0   0 106 124 122 129 140 143 131 174\n",
      "  107   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0 255 255 255 255 255 255 255 255\n",
      "  255   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  14 255 245 241 241 238 240 238 242\n",
      "  255   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  33 255 243 251 250 247 246 248 242\n",
      "  255  21   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0 103 255 240 248 248 247 246 245 241\n",
      "  255 100   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0 158 255 240 248 248 248 246 246 239\n",
      "  255 141   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0 182 255 241 249 248 253 248 246 239\n",
      "  255 169   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0 179 255 237 243 254 252 255 246 240\n",
      "  255 177   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0 146 255 237 245 255  83 255 251 236\n",
      "  255 140   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0 105 255 235 249 255   0 255 255 240\n",
      "  255 105   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  61 255 240 254 255   0 247 255 239\n",
      "  255  64   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  12 255 239 249 237   0 217 255 242\n",
      "  255  10   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0 255 241 253 208   0 182 255 243\n",
      "  255   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0 255 247 255 178   0 162 255 245\n",
      "  252   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0 215 252 255 149   0 133 255 253\n",
      "  203   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0 148 255 255 141   0 124 255 255\n",
      "  156   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0 126 255 254 158   0 128 255 255\n",
      "  149   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0 105 255 254 137   0  90 255 255\n",
      "  151   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0 111 255 250  67   0  14 247 255\n",
      "  166   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0 145 255 250  39   0   0 230 255\n",
      "  202   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0 149 255 250  44   0   0 223 255\n",
      "  215   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0 127 255 251  41   0   0 217 255\n",
      "  192   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0  65 255 244   9   0   0 198 255\n",
      "  142   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0  11 254 239   0   0   0 172 255\n",
      "  123   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0 247 244   1   0   0 161 255\n",
      "  111   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0 224 239   0   0   0 143 255\n",
      "   83   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0 215 251   0   0   0 151 255\n",
      "   82   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0 136 150   0   0   0  54 179\n",
      "   15   0   0   0   0   0   0   0   0   0]]\n",
      "--------------------\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Run this cell a few times to randomly display some digit data\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "index = random.randint(0, len(train_images))\n",
    "# index = 100\n",
    "print (\"Displaying train index = \", index)\n",
    "\n",
    "print(\"train label [{}] = {} \".format(index, train_labels[index]))\n",
    "print (\"------------ raw data for train_image[{}] -------\".format(index))\n",
    "print(train_images[index])\n",
    "print (\"--------------------\")\n",
    "\n",
    "plt.imshow(train_images[index])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Shape data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images  = train_images / 255.0\n",
    "test_images = test_images / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Create model\n",
    "\n",
    "### TODO : Sketch the neural net\n",
    "- What is the input dimensions\n",
    "- how many neurons in layers\n",
    "- how many output neurons\n",
    "\n",
    "<img src=\"../assets/images/neural-net-unknown.png\" style=\"width:30%\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 101,770\n",
      "Trainable params: 101,770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "## Model 1\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "            tf.keras.layers.Dense(128, activation=tf.nn.relu), \n",
    "            tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "            ])\n",
    "\n",
    "\n",
    "model.compile(\n",
    "  optimizer=tf.keras.optimizers.Adam(),  # 'adam'\n",
    "  loss='sparse_categorical_crossentropy',\n",
    "  metrics=['accuracy'])\n",
    "\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 - Setup Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TB logs to :  /tmp/tensorboard-logs/fashion-mnist/18-44-33\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-18443bd3b5ba3eb5\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-18443bd3b5ba3eb5\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6012;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## This is fairly boiler plate code\n",
    "\n",
    "import datetime\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "app_name = 'fashion-mnist'\n",
    "\n",
    "# timestamp  = datetime.datetime.now().strftime(\"%Y-%m-%d--%H-%M-%S\")\n",
    "\n",
    "tb_top_level_dir= '/tmp/tensorboard-logs'\n",
    "\n",
    "tb_app_dir = os.path.join (tb_top_level_dir, app_name)\n",
    "\n",
    "tb_logs_dir = os.path.join (tb_app_dir, datetime.datetime.now().strftime(\"%H-%M-%S\"))\n",
    "\n",
    "\n",
    "print (\"Saving TB logs to : \" , tb_logs_dir)\n",
    "\n",
    "#clear out old logs\n",
    "shutil.rmtree ( tb_app_dir, ignore_errors=True )\n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=tb_logs_dir, write_graph=True, \n",
    "                                                      write_images=True, histogram_freq=1)\n",
    "## This will embed Tensorboard right here in jupyter!\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir $tb_logs_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Training the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training starting ...\n",
      "Epoch 1/10\n",
      "1500/1500 [==============================] - 11s 7ms/step - loss: 0.5124 - accuracy: 0.8222 - val_loss: 0.4311 - val_accuracy: 0.8446\n",
      "Epoch 2/10\n",
      "1500/1500 [==============================] - 12s 8ms/step - loss: 0.3855 - accuracy: 0.8618 - val_loss: 0.3574 - val_accuracy: 0.8702\n",
      "Epoch 3/10\n",
      "1500/1500 [==============================] - 13s 8ms/step - loss: 0.3413 - accuracy: 0.8762 - val_loss: 0.3637 - val_accuracy: 0.8680\n",
      "Epoch 4/10\n",
      "1500/1500 [==============================] - 15s 10ms/step - loss: 0.3176 - accuracy: 0.8843 - val_loss: 0.3470 - val_accuracy: 0.8754\n",
      "Epoch 5/10\n",
      "1500/1500 [==============================] - 9s 6ms/step - loss: 0.2983 - accuracy: 0.8890 - val_loss: 0.3321 - val_accuracy: 0.8804\n",
      "Epoch 6/10\n",
      "1500/1500 [==============================] - 9s 6ms/step - loss: 0.2795 - accuracy: 0.8957 - val_loss: 0.3532 - val_accuracy: 0.8752\n",
      "Epoch 7/10\n",
      "1500/1500 [==============================] - 8s 5ms/step - loss: 0.2700 - accuracy: 0.9006 - val_loss: 0.3343 - val_accuracy: 0.8831\n",
      "Epoch 8/10\n",
      "1500/1500 [==============================] - 13s 9ms/step - loss: 0.2604 - accuracy: 0.9027 - val_loss: 0.3270 - val_accuracy: 0.8852\n",
      "Epoch 9/10\n",
      "1500/1500 [==============================] - 13s 9ms/step - loss: 0.2496 - accuracy: 0.9075 - val_loss: 0.3326 - val_accuracy: 0.8852\n",
      "Epoch 10/10\n",
      " 792/1500 [==============>...............] - ETA: 6s - loss: 0.2367 - accuracy: 0.9108"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "epochs=10\n",
    "print (\"training starting ...\")\n",
    "\n",
    "history = model.fit(\n",
    "    train_images,\n",
    "    train_labels,\n",
    "    epochs=epochs,\n",
    "    validation_split=0.2,\n",
    "    verbose=1,\n",
    "    callbacks=[tensorboard_callback]\n",
    ")\n",
    "\n",
    "print (\"training done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7 : See Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='train_accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8 - Predict\n",
    "\n",
    "**==> Compare prediction time vs training time.  Prediction is very quick!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "print (\"predicting on {:,} images\".format(len(test_images)))\n",
    "predictions = model.predict(test_images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Print a sample prediction\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "np.set_printoptions(formatter={'float': '{: 0.3f}'.format})\n",
    "\n",
    "index = random.randint(0, len(test_images))\n",
    "\n",
    "print (\"random index = \", index)\n",
    "print (\"test_label[{}] = {}.  So the number is {}\".format(index, test_labels[index], test_labels[index]))\n",
    "print (\"prediction of test_image[{}] = {}\".format(index, predictions[index]))\n",
    "print ('max softmax output = ', np.amax(predictions[index]))\n",
    "print ('index of max softmax output = {}.  So the prediction is same ({})'.format(np.argmax(predictions[index]), np.argmax(predictions[index])))\n",
    "\n",
    "plt.imshow(test_images2[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9 : Evaluate the Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1 - Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_names = model.metrics_names\n",
    "print (\"model metrics : \" , metric_names)\n",
    "\n",
    "metrics = model.evaluate(test_images, test_labels, verbose=0)\n",
    "\n",
    "for idx, metric in enumerate(metric_names):\n",
    "    print (\"Metric : {} = {:,.3f}\".format (metric_names[idx], metrics[idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 - Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## our predictions is an array of arrays\n",
    "print('predictions shape : ', predictions.shape)\n",
    "print ('prediction 0 : ' , predictions[0])\n",
    "print ('prediction 1 : ' , predictions[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We need to find the final output (max of softmax probabilities for each prediction)\n",
    "predictions2 = [ np.argmax(p) for p in predictions]\n",
    "print ('prediction2 0 : ' , predictions2[0])\n",
    "print ('prediction2 1 : ' , predictions2[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "cm = confusion_matrix(test_labels, predictions2, labels = [0,1,2,3,4,5,6,7,8,9])\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize = (8,6))\n",
    "\n",
    "# colormaps : cmap=\"YlGnBu\" , cmap=\"Greens\", cmap=\"Blues\",  cmap=\"Reds\"\n",
    "sns.heatmap(cm, annot=True, cmap=\"Reds\", fmt='d').plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3 - Metrics Calculated from Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from pprint import pprint\n",
    "\n",
    "pprint(classification_report(test_labels, predictions2, output_dict=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10 : Improve the Model\n",
    "\n",
    "Try the following exercises\n",
    "\n",
    "**1 - Increase epochs**  \n",
    "- In Step-6, increase epochs to 50\n",
    "- Rerun the notebook\n",
    "- did the accuracy improve?\n",
    "- how much the training time go up?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
